{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d620117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Needed Packages\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import math\n",
    "\n",
    "BLINK_RATIO_THRESHOLD = 5.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e73692e",
   "metadata": {},
   "source": [
    "The function get_blink_ratio returns the value of the blink ratio of one eye. It has two arguments, eye_points is the array of indexes of the eye landmarks (part 4.1) and dlib.full_object_detection object discussed in part 4.2.\n",
    "We now extract the points required to calculate the horizontal and vertical lengths. corner_left and corner_right are the points used to calculate the horizontal length. We directly extract them from the facial landmarks as explained in part 4.3. Unlike corner_left and corner_right we have to calculate center_top and center_bottom from the facial landmarks available. center_top and center_bottom are the midpoints of the two eye landmarks on the top and the bottom respectively. They are used to calculate the vertical length.\n",
    "After extracting all the points, we calculate the length using the euclidean_distance function to get horizontal_length and vertical_length. Then we calculate and then return the blink eye ratio of one eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef74850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting to know Blink Ratio\n",
    "\n",
    "\n",
    "def midpoint(point1, point2):\n",
    "    return int((point1.x + point2.x)/2), int((point1.y + point2.y)/2)\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "def get_blink_ratio(eye_points, facial_landmarks):\n",
    "    \n",
    "    #loading all the required points\n",
    "    corner_left  = (facial_landmarks.part(eye_points[0]).x, \n",
    "                    facial_landmarks.part(eye_points[0]).y)\n",
    "    corner_right = (facial_landmarks.part(eye_points[3]).x, \n",
    "                    facial_landmarks.part(eye_points[3]).y)\n",
    "    \n",
    "    center_top    = midpoint(facial_landmarks.part(eye_points[1]), \n",
    "                             facial_landmarks.part(eye_points[2]))\n",
    "    center_bottom = midpoint(facial_landmarks.part(eye_points[5]), \n",
    "                             facial_landmarks.part(eye_points[4]))\n",
    "\n",
    "    #calculating distance\n",
    "    horizontal_length = euclidean_distance(corner_left,corner_right)\n",
    "    vertical_length = euclidean_distance(center_top,center_bottom)\n",
    "\n",
    "    if(vertical_length == 0):\n",
    "        vertical_length = 0.0000001\n",
    "    \n",
    "    ratio = horizontal_length / vertical_length\n",
    "\n",
    "        \n",
    "        \n",
    "    return ratio\n",
    "\n",
    "def get_blink_ratio(eye_points, facial_landmarks):\n",
    "    \n",
    "    #loading all the required points\n",
    "    corner_left  = (facial_landmarks.part(eye_points[0]).x, \n",
    "                    facial_landmarks.part(eye_points[0]).y)\n",
    "    corner_right = (facial_landmarks.part(eye_points[3]).x, \n",
    "                    facial_landmarks.part(eye_points[3]).y)\n",
    "    \n",
    "    center_top    = midpoint(facial_landmarks.part(eye_points[1]), \n",
    "                             facial_landmarks.part(eye_points[2]))\n",
    "    center_bottom = midpoint(facial_landmarks.part(eye_points[5]), \n",
    "                             facial_landmarks.part(eye_points[4]))\n",
    "\n",
    "    #calculating distance\n",
    "    horizontal_length = euclidean_distance(corner_left,corner_right)\n",
    "    vertical_length = euclidean_distance(center_top,center_bottom)\n",
    "\n",
    "    if(vertical_length == 0):\n",
    "        vertical_length = 0.0000001\n",
    "    \n",
    "    ratio = horizontal_length / vertical_length\n",
    "\n",
    "        \n",
    "        \n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n",
    "def eyes_movement_left_right(eye_points1, eye_points2, facial_landmarks):\n",
    "    \n",
    "    corner_left1  = (facial_landmarks.part(eye_points1[0]).x, \n",
    "                    facial_landmarks.part(eye_points1[0]).y)\n",
    "    corner_right1 = (facial_landmarks.part(eye_points1[3]).x, \n",
    "                    facial_landmarks.part(eye_points1[3]).y)\n",
    "    \n",
    "    corner_left2  = (facial_landmarks.part(eye_points2[0]).x, \n",
    "                    facial_landmarks.part(eye_points2[0]).y)\n",
    "    corner_right2 = (facial_landmarks.part(eye_points2[3]).x, \n",
    "                    facial_landmarks.part(eye_points2[3]).y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    d_left = euclidean_distance(corner_left1,corner_right1)\n",
    "    \n",
    "    d_right = euclidean_distance(corner_left2,corner_right2)\n",
    "    \n",
    "    \n",
    "    if((d_left - d_right) > 0.1):\n",
    "        return 1\n",
    "    elif( (d_right - d_left) > 0.1):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "468253ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't receive frame (stream end?). Exiting ...\n"
     ]
    }
   ],
   "source": [
    "# Capturing Video or Webcam\n",
    "\n",
    "cap = cv2.VideoCapture(\"rec.mp4\")\n",
    "\n",
    "# Giving the name of the Displaying Window\n",
    "\n",
    "cv2.namedWindow(\"Blink Detector\")\n",
    "\n",
    "'''We are now going to use dlib’s default face detector to detect all the faces in the image. \n",
    "This face detection model is based on HoG and SVM. \n",
    "We first load the detector using the get_frontal_face_detector().'''\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Detecting Eyes Using Landmarks in dlib\n",
    "\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "#left and right eye landmarks\n",
    "\n",
    "left_eye_landmarks = [36, 37, 38, 39, 40, 41]\n",
    "right_eye_landmarks = [42, 43, 44, 45, 46, 47]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Capturing Frames from video stream\n",
    "\n",
    "while True:\n",
    "    # While Loop - To decide for how long you want to keep capturing the stream\n",
    "    \n",
    "    if_captured, frame = cap.read()\n",
    "    \n",
    "    # Exit the application if frame not found... Video is not being recorded for some reason... or if there is some technical\n",
    "    # glitch\n",
    "    \n",
    "    if not if_captured:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "    '''The frame we have captured is a 3-channel RGB colored image. \n",
    "    We detect face and eyes in the frame. \n",
    "    dlib’s face detection works perfectly fine on grayscale images as well as colored images. \n",
    "    As grayscale image is a single channel image, we convert the frame to grayscale to reduce to the processing \n",
    "    time required by further steps of the algorithm.\n",
    "    We use cv2.cvtColor(frame, flag) for color conversion. \n",
    "    Here we use the flag cv2.COLOR_BGR2GRAY to convert the colored image to grayscale.'''\n",
    "    \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    \n",
    "    # Face Detection with Dlib\n",
    "    \n",
    "    '''We are now going to use dlib’s default face detector to detect all the faces in the image. \n",
    "    This face detection model is based on HoG and SVM. We first load the detector using the get_frontal_face_detector(). \n",
    "    Then we pass the image in the detector as the first argument. \n",
    "    The second argument is the number of times we want to upscale the image.\n",
    "    The third argument is the value of the threshold. \n",
    "    In our application, we have used the default values for these arguments.\n",
    "    This detector returns a list of dlib.rectangle objects which are the bounding boxes of the faces detected in the frame.'''\n",
    "    \n",
    "    faces,_,_ = detector.run(image = frame)\n",
    "    \n",
    "    for face in faces:\n",
    "        \n",
    "        # Python3 code to demonstrate\n",
    "        # getting numbers from string\n",
    "        # using re.findall()\n",
    "        import re\n",
    "\n",
    "        # initializing string\n",
    "        test_string = str(face)\n",
    "\n",
    "        # printing original string\n",
    "    #     print(\"The original string : \" + test_string)\n",
    "\n",
    "        # using re.findall()\n",
    "        # getting numbers from string\n",
    "        temp = re.findall(r'\\d+', test_string)\n",
    "        res = list(map(int, temp))\n",
    "\n",
    "        # print result\n",
    "    #     print(\"The numbers list is : \" + str(res))\n",
    "        # Blue color in BGR\n",
    "        color = (255, 0, 0)\n",
    "\n",
    "        # Line thickness of 2 px\n",
    "        thickness = 2\n",
    "\n",
    "\n",
    "        # Using cv2.rectangle() method\n",
    "        # Draw a rectangle with blue line borders of thickness of 2 px\n",
    "        frame = cv2.rectangle(frame, (res[0] , res[1]), (res[2],res[3]), color, int(2))\n",
    "\n",
    "    #   Detecting Eyes using dlib\n",
    "        \n",
    "        landmarks = predictor(frame, face)\n",
    "        \n",
    "        #-----Step 5: Calculating blink ratio for one eye-----\n",
    "        left_eye_ratio  = get_blink_ratio(left_eye_landmarks, landmarks)\n",
    "        right_eye_ratio = get_blink_ratio(right_eye_landmarks, landmarks)\n",
    "        blink_ratio     = (left_eye_ratio + right_eye_ratio) / 2\n",
    "        \n",
    "        if blink_ratio > BLINK_RATIO_THRESHOLD:\n",
    "            #Blink detected! Do Something!\n",
    "            cv2.putText(frame,\"He is BLINKING\",(10,50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        2,(0,0,0),2,cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        for i in range(len(left_eye_landmarks)):\n",
    "            points1 = (landmarks.part(left_eye_landmarks[i]).x, landmarks.part(left_eye_landmarks[i]).y)\n",
    "            points2 = (landmarks.part(right_eye_landmarks[i]).x, landmarks.part(right_eye_landmarks[i]).y)\n",
    "    \n",
    "            frame = cv2.circle(frame, points1, 1, color, thickness )\n",
    "            frame = cv2.circle(frame, points2, 1, color, thickness)\n",
    "            \n",
    "         \n",
    "        sig = eyes_movement_left_right(left_eye_landmarks, right_eye_landmarks, landmarks)\n",
    "        if(sig == 1):\n",
    "            cv2.putText(frame,\"Eyes moving Left\",(5,70), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        2,(0,0,0),2,cv2.LINE_AA)\n",
    "        elif(sig == -1):\n",
    "            cv2.putText(frame,\"Eyes Moving Right\",(5,70), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        2,(0,0,0),2,cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(frame,\"He is Facing Forward\",(5,70), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        2,(0,0,0),2,cv2.LINE_AA)\n",
    "            \n",
    "    \n",
    "    # Let's watch the stream being captured in real time.\n",
    "    \n",
    "    cv2.imshow('Blink Detector', frame)\n",
    "    # Here we are using imshow function to watch the stream being captured. It is taking the window name (in which you want to\n",
    "    # to watch the stream) and the frame variable storing the current frame of the stream being captured.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now the code for closing the stream when needed.\n",
    "    key = cv2.waitKey(1)\n",
    "    #  We use cv2.waitKey()to detect key press.\n",
    "    \n",
    "    if(key == 27): #27 is the key code for the escape key. We stop capturing the frames once the escape key is pressed\n",
    "        break\n",
    "    \n",
    "    \n",
    "    \n",
    "# Now as we are exiting the while loop, we need to close capturing the video.\n",
    "\n",
    "cap.release() # release software resource; release hardware resource. \n",
    "cv2.destroyAllWindows() # closes all of the imshow() windows.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693f661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
